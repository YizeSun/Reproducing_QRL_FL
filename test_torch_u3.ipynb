{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister, assemble, Aer\n",
    "import torch\n",
    "from torch import nn\n",
    "from math import pi\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.distributions.Uniform(0, 0.01).sample((4,3)))\n",
    "        self.bias = nn.Parameter(torch.zeros(4))\n",
    "        self.circuit_size = 4\n",
    "        self.n_actions = 4\n",
    "        self.shots = 1000\n",
    "    \n",
    "    def _circuitMaker(self, n_qubits, encodings, params, measurement, n_cbits=1):\n",
    "        # create quantum circuit\n",
    "        qr = QuantumRegister(n_qubits, 'qubit')\n",
    "        cr = ClassicalRegister(n_cbits, 'cr')\n",
    "        qc = QuantumCircuit()\n",
    "        qc.add_register(qr)\n",
    "        qc.add_register(cr)\n",
    "        \n",
    "        # encoding block\n",
    "        encoder0, encoder1 = self._encoder(encodings)\n",
    "        # print(encodings, encoder1)\n",
    "        # qc.rx(pi, encoder0)\n",
    "        # qc.rz(pi, encoder0)\n",
    "        if encoder1:\n",
    "            qc.rx(pi, encoder1)\n",
    "            qc.rz(pi, encoder1)\n",
    "            qc.barrier(qr)\n",
    "        \n",
    "        for i in range(n_qubits):\n",
    "            # entangle block\n",
    "            if i < n_qubits - 1:\n",
    "                qc.cnot(i, i+1)\n",
    "            else:\n",
    "                qc.barrier(qr)\n",
    "\n",
    "        for i in range(n_qubits):\n",
    "            # variational block\n",
    "            qc.u3(params[i][0], params[i][1], params[i][2], i)\n",
    "\n",
    "        # measurement\n",
    "        for i, j in enumerate(measurement):\n",
    "            qc.measure(qr[j], cr[i])\n",
    "        \n",
    "        return qc\n",
    "\n",
    "    def _encoder(self, encodings):\n",
    "        return [i for i, b in enumerate(encodings) if b == '0'], [i for i, b in enumerate(encodings) if b == '1']\n",
    "\n",
    "    def _getAction_single_measure(self, state):\n",
    "        b_state = f'{state:0{self.circuit_size}b}'\n",
    "        # tasks = [circuitMaker(circuit_size, b_state, parameters, [i]) for i in range(n_actions)]\n",
    "        aer_sim = Aer.get_backend('aer_simulator')\n",
    "        param_current = self.weights.detach().numpy()\n",
    "        rlt = [aer_sim.run(assemble(self._circuitMaker(self.circuit_size, b_state, param_current, [i]), shots=self.shots)).result().get_counts() for i in range(self.n_actions)]\n",
    "        # qobj = assemble(qc, shots=shots)\n",
    "        # job = aer_sim.run(qobj)\n",
    "        # rlt = job.result().get_counts()\n",
    "        # print(rlt)\n",
    "        q_exps = [(i['0']-i['1'])/self.shots if len(i) > 1 else (-i['1']/self.shots if '1' in i.keys() else i['0']/self.shots) for i in rlt]\n",
    "\n",
    "        return [a + b for a, b in zip(q_exps, self.bias.detach().numpy())]\n",
    "\n",
    "    def forward(self, stac):\n",
    "        return torch.tensor([self._getAction_single_measure(state)[action] for state, action in stac])\n",
    "\n",
    "sampled_data = {\n",
    "    'state':[0, 3, 4, 8, 14], \n",
    "    'action':[1, 0, 2, 3, 2], \n",
    "    'reward':[-0.01, -0.01, -0.2, -0.01, 1], \n",
    "    'new_state':[4, 2, 5, 4, 15], \n",
    "    'done':[0, 0, 1, 0, 1]\n",
    "    }\n",
    "\n",
    "sampled_df = pd.DataFrame(sampled_data)\n",
    "model_pred = Model()\n",
    "model_tar = Model()\n",
    "opt = torch.optim.Adam(params=model_pred.parameters(), lr=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights tensor([[0.0077, 0.0034, 0.0029],\n",
      "        [0.0023, 0.0005, 0.0048],\n",
      "        [0.0073, 0.0039, 0.0048],\n",
      "        [0.0044, 0.0051, 0.0003]])\n",
      "bias tensor([0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XINYAN~1\\AppData\\Local\\Temp/ipykernel_18524/1584971010.py:37: DeprecationWarning: The QuantumCircuit.u3 method is deprecated as of 0.16.0. It will be removed no earlier than 3 months after the release date. You should use QuantumCircuit.u instead, which acts identically. Alternatively, you can decompose u3 in terms of QuantumCircuit.p and QuantumCircuit.sx: u3(ϴ,φ,λ) = p(φ+π) sx p(ϴ+π) sx p(λ) (2 pulses on hardware).\n",
      "  qc.u3(params[i][0], params[i][1], params[i][2], i)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\XINYAN~1\\AppData\\Local\\Temp/ipykernel_18524/1649297138.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\annaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\annaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.1\n",
    "# TODO: check how to define class model_tar->forward->return pytorch.tensor [(sampled_df['reward'][i] if sampled_df['done'][i] else sampled_df['reward'][i] + GAMMA*np.max(tar)) for i in sampled_df.index]\n",
    "# check this way:https://towardsdatascience.com/integrating-tensorflow-and-qiskit-for-quantum-machine-learning-7fa6b14d5294\n",
    "# this:https://medium.com/qiskit/introducing-qiskit-machine-learning-5f06b6597526\n",
    "# (https://www.reddit.com/r/QuantumComputing/comments/mm5urd/qiskit_with_pytorch_has_arrived/)\n",
    "# First see:\n",
    "# https://github.com/Qiskit/qiskit-machine-learning\n",
    "# https://qiskit.org/documentation/machine-learning/tutorials/05_torch_connector.html\n",
    "pred = model_pred.forward(zip(sampled_df['state'], sampled_df['action']))\n",
    "# tar = model_tar.forward(zip(sampled_df['state'], sampled_df['action']))\n",
    "# tar = [(sampled_df['reward'][i] if sampled_df['done'][i] else sampled_df['reward'][i] + GAMMA*np.max(tar)) for i in sampled_df.index]\n",
    "\n",
    "for name, param in model_pred.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(pred, pred)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "\n",
    "print('after one step optimize')\n",
    "\n",
    "for name, param in model_pred.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "weights = torch.distributions.Uniform(0, 0.01).sample((4,3))\n",
    "bias = torch.zeros(4)\n",
    "##params_q = nn.Parameter(weights)\n",
    "params_bias = nn.Parameter(bias)\n",
    "print(params_bias.detach().numpy())\n",
    "#print(params_q.detach().numpy()) # CPU tensor\n",
    "# print(params_q.detach().to('cpu').numpy()) # CUDA tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cost(params, sampled_vs, q_target):\n",
    "    # global sampled_vs, q_target\n",
    "    v_targets = [(s[2] if s[4] else s[2] + GAMMA*np.max(q_target)) for s in sampled_vs]\n",
    "    v_predics = [getAction_single_measure(s[0], params)[s[1]] for s in sampled_vs]\n",
    "    cost = sum([(tar - pred)**2 for tar, pred in zip(v_targets, v_predics)])/MINIBATCH # MSE\n",
    "    # print('cost: ', cost)\n",
    "    return cost\n",
    "\n",
    "n_qubits = 4\n",
    "n_cbits = 1\n",
    "n_actions = 4\n",
    "circuit_size = 4\n",
    "shots = 1000\n",
    "GAMMA = 0.9\n",
    "MINIBATCH = 5\n",
    "\n",
    "qc = circuitMaker(n_qubits, '1011', [1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0], [0])\n",
    "qc.draw()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
